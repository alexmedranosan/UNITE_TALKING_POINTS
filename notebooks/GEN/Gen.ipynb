{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f44c5a-91b6-4481-9d4b-20e6a9463863",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "For this type of system we will carry out the same experimentation using no context and different generation models, GPT-3.5-turbo, GPT-4, GPT-4o and GPT-4-mini.\n",
    "\n",
    "To measure the quality of the system, we will use the defined metric TPPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ec7f6-ca40-4121-9b3a-2fc194d67070",
   "metadata": {},
   "source": [
    "## 0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3493fe31-751a-4ebc-82d4-2611dcc24b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to home project directory\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d502c70b-be9e-4dc0-a85b-1c7f43470260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "from src.unite_talking_points.utils.config.config_loader import ConfigLoader\n",
    "from typing import TypedDict\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cca0ec-c44a-40dd-883c-637dde589adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigLoader().load_config(current_directory_is_root=True)\n",
    "os.environ[\"OPENAI_API_KEY\"] = config['External-services']['openai_api_key']\n",
    "client = openai.OpenAI(api_key=config['External-services']['openai_api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e3650c-a5ed-4e44-a6a3-6e41979b9b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPPIResult(TypedDict):\n",
    "    TPPI: float\n",
    "    BertScore: float\n",
    "    Normalized_BertScore: float\n",
    "    Perplexity: float\n",
    "    Normalized_Perplexity: float\n",
    "    Flesch: float\n",
    "    Normalized_Flesch: float\n",
    "\n",
    "\n",
    "class TPPI:\n",
    "    def __init__(self, model_type: str = 'bert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Inicializa el normalizador de puntuaciones y los modelos necesarios para los cálculos.\n",
    "        \"\"\"\n",
    "        self.scorer = BERTScorer(model_type=model_type)\n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "        # Set the maximum lenght for GPT-2\n",
    "        self.perplexity_max_length = 1024\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_bert_score(bert_score: float) -> float:\n",
    "        \"\"\"\n",
    "        Normaliza el puntaje BERT en un rango de 0 a 1.\n",
    "        \"\"\"\n",
    "        return (bert_score + 1) / 2\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_perplexity(perplexity: float) -> float:\n",
    "        \"\"\"\n",
    "        Normaliza el valor de la perplejidad en un rango de 0 a 1.\n",
    "        \"\"\"\n",
    "        # Aseguramos que la perplexidad mínima sea al menos 1 para evitar división por cero\n",
    "        perplexity = max(perplexity, 1)\n",
    "        # Invertimos la fórmula para que valores bajos de perplexidad den valores altos normalizados\n",
    "        return 1 - (min(perplexity, 100) - 1) / 99\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_flesch(flesch: float) -> float:\n",
    "        \"\"\"\n",
    "        Normaliza el puntaje de Flesch en un rango de 0 a 1.\n",
    "        \"\"\"\n",
    "        return max(min(flesch, 100), 0) / 100\n",
    "\n",
    "    def calculate_bert_score(self, reference_text: str, generated_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula el BertScore entre un texto de referencia y uno generado.\n",
    "        \"\"\"\n",
    "        P, R, F1 = self.scorer.score([reference_text], [generated_text], verbose=False)\n",
    "        bert_score = F1.item()\n",
    "        return bert_score\n",
    "\n",
    "    def calculate_perplexity(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula la perplejidad de un texto utilizando el modelo GPT-2.\n",
    "        \"\"\"\n",
    "        # Tokenize the input text\n",
    "        encode = self.gpt2_tokenizer.encode(text, return_tensors='pt')\n",
    "    \n",
    "        # Trim the input if it exceeds the maximum length\n",
    "        if encode.size(1) > self.perplexity_max_length:  # Check the token sequence length\n",
    "            encode = encode[:, :self.perplexity_max_length]  # Trim to the first max_length tokens\n",
    "    \n",
    "        # Calculate the perplexity\n",
    "        with torch.no_grad():\n",
    "            loss = self.gpt2_model(encode, labels=encode)[0]\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        \n",
    "        return perplexity\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_flesch(text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula el puntaje de Flesch Reading Ease de un texto.\n",
    "        \"\"\"\n",
    "        flesch_score = flesch_reading_ease(text)\n",
    "        return flesch_score\n",
    "\n",
    "    def calculate_tppi(self, original_text: str, generated_text: str) -> TPPIResult:\n",
    "        \"\"\"\n",
    "        Calcula el TPPI basado en los textos de referencia y generados, devolviendo los puntajes de BERT,\n",
    "        perplejidad y Flesch tanto normalizados como sin normalizar.\n",
    "        \"\"\"\n",
    "        # Calcula BertScore\n",
    "        bert_score = self.calculate_bert_score(original_text, generated_text)\n",
    "        normalized_bert_score = self.normalize_bert_score(bert_score)\n",
    "\n",
    "        # Calcula Perplejidad\n",
    "        perplexity = self.calculate_perplexity(generated_text)\n",
    "        normalized_perplexity = self.normalize_perplexity(perplexity)\n",
    "\n",
    "        # Calcula Flesch\n",
    "        flesch = self.calculate_flesch(generated_text)\n",
    "        normalized_flesch = self.normalize_flesch(flesch)\n",
    "\n",
    "        # Calcula TPPI\n",
    "        tppi_score = 0.5 * normalized_bert_score + 0.25 * normalized_perplexity + 0.25 * normalized_flesch\n",
    "\n",
    "        # Devuelve todos los valores en un TypedDict\n",
    "        return TPPIResult(\n",
    "            TPPI=tppi_score,\n",
    "            BertScore=bert_score,\n",
    "            Normalized_BertScore=normalized_bert_score,\n",
    "            Perplexity=perplexity,\n",
    "            Normalized_Perplexity=normalized_perplexity,\n",
    "            Flesch=flesch,\n",
    "            Normalized_Flesch=normalized_flesch\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46500b13-7266-4cff-9868-31a5d1fa1d78",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb0406-e9fa-4444-a2b4-6331d04ea7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load JSON data from the specified directory into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - data_dir (str): Path to the directory containing JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): DataFrame containing the loaded JSON data.\n",
    "    \"\"\"\n",
    "    # Initialize empty lists to store data\n",
    "    file_names = []\n",
    "    labels = []\n",
    "    document_names = []\n",
    "    meeting_names = []\n",
    "    meeting_dates = []\n",
    "    contents = []\n",
    "    prompts = []\n",
    "\n",
    "    # Iterate over each JSON file in the directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(data_dir, filename), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                # Extract data from each JSON file and append to lists\n",
    "                file_names.append(filename)\n",
    "                labels.append(data['label'])\n",
    "                document_names.append(data['document_name'])\n",
    "                meeting_names.append(data['meeting_name'])\n",
    "                meeting_dates.append(data['meeting_date'])\n",
    "                contents.append(data['content'])\n",
    "                prompts.append(data['prompt'])\n",
    "\n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        'file_name': file_names,\n",
    "        'label': labels,\n",
    "        'document_name': document_names,\n",
    "        'meeting_name': meeting_names,\n",
    "        'meeting_date': meeting_dates,\n",
    "        'content': contents,\n",
    "        'prompt': prompts\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27dd597c-2427-485d-a0d0-ab775c301483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_json_data(config['Directories']['raw_data_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d710505a-f6bb-405a-b34c-8ff34749bb96",
   "metadata": {},
   "source": [
    "## 2. Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b13819-3589-466d-bb38-57084b7f9166",
   "metadata": {},
   "source": [
    "### 2.1 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d12b3-d0bf-4749-b3b6-f041793a9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_talking_point(prompt, model=\"gpt-4o-mini\"):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce064c08-80b1-4a2e-9ac0-b9e80b546518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TPPI scoring system\n",
    "tppi = TPPI()\n",
    "\n",
    "# Function to conduct n experiments without averaging results\n",
    "def conduct_experiment(df, model=\"gpt-4o-mini\", n=3):\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        \n",
    "        prompt = df.iloc[i].prompt\n",
    "        content = df['content'][i]\n",
    "        file_name = df['file_name'][i]\n",
    "\n",
    "        # Conduct n experiments\n",
    "        for experiment_num in range(n):\n",
    "            # Generate talking point\n",
    "            generated_talking_point = generate_talking_point(prompt, model=model)\n",
    "\n",
    "            # Calculate TPPI\n",
    "            tppi_result = tppi.calculate_tppi(content, generated_talking_point)\n",
    "            \n",
    "            # Add metadata fields to each experiment result\n",
    "            tppi_result['file_name'] = file_name\n",
    "            tppi_result['model'] = model\n",
    "            tppi_result['experiment_number'] = experiment_num + 1\n",
    "\n",
    "            results.append(tppi_result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e551952-5616-404e-84ac-6decefb12cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "models = ['gpt-3.5-turbo', 'gpt-4o-mini', 'gpt-4o']\n",
    "all_results = []\n",
    "\n",
    "for model in models:\n",
    "    all_results.append(conduct_experiment(df, model, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93f2da-21ac-4679-ab5c-7c0b4c4cb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = [item for sublist in all_results for item in sublist]\n",
    "all_results = pd.DataFrame(flattened_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8760626-446b-44f5-ab6c-cf31125f9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula estadísticas básicas para cada método\n",
    "statistics = all_results.groupby('model').agg({\n",
    "    'TPPI': ['mean', 'std', 'min', 'max'],\n",
    "    'BertScore': ['mean'],\n",
    "    'Perplexity': ['mean'],\n",
    "    'Flesch' : ['mean']\n",
    "}).reset_index()\n",
    "\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df470b6-4083-4705-b9cf-7ea85802566f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los nombres de las métricas para iterar\n",
    "metrics = ['TPPI', 'Normalized_BertScore', 'Normalized_Perplexity', 'Normalized_Flesch']\n",
    "\n",
    "# Crear un subplot 2x2 para los boxplots de cada métrica\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "fig.suptitle('Distribución de Métricas por Método de Vectorización', fontsize=16)\n",
    "\n",
    "# Iterar sobre las métricas y los ejes para crear los boxplots\n",
    "for ax, metric in zip(axes.flatten(), metrics):\n",
    "    all_results.boxplot(column=metric, by='model', ax=ax, grid=False)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('Modelo')\n",
    "    ax.set_ylim(0, 1)  # Asegurar que el gráfico esté limitado entre 0 y 1\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)  # Rotar las etiquetas para mejor legibilidad\n",
    "\n",
    "# Ajustar el layout para evitar la superposición de los títulos y etiquetas\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Ajusta los límites del rectángulo del layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38aa4b2f-a911-4872-8c7d-eae62ae21408",
   "metadata": {},
   "source": [
    "### 2.2 Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1ba3aa-277f-4c16-b079-279e4d6c366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_talking_point(prompt, model=\"gpt-4o-mini\", temperature=1.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    answer = response.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "\n",
    "def conduct_experiment(df, model=\"gpt-4o-mini\", temperatures=[0, 0.5, 1.0, 1.5, 2], n=3):\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(len(df))):\n",
    "        prompt = df.iloc[i].prompt\n",
    "        content = df['content'][i]\n",
    "        file_name = df['file_name'][i]\n",
    "\n",
    "        # Iterate over each temperature value\n",
    "        for temp in tqdm(temperatures, leave=False):\n",
    "            # Conduct n experiments for each temperature\n",
    "            for experiment_num in tqdm(range(n), leave=False):\n",
    "                # Generate talking point with specific temperature\n",
    "                generated_talking_point = generate_talking_point(prompt, model=model, temperature=temp)\n",
    "\n",
    "                # Calculate TPPI\n",
    "                tppi_result = tppi.calculate_tppi(content, generated_talking_point)\n",
    "\n",
    "                # Add metadata fields to each experiment result\n",
    "                tppi_result['file_name'] = file_name\n",
    "                tppi_result['temperature'] = temp\n",
    "                tppi_result['experiment_number'] = experiment_num + 1\n",
    "\n",
    "                results.append(tppi_result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fd04c8-6a3f-4217-957d-d3b37e9ebbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = conduct_experiment(df, model=\"gpt-3.5-turbo\", temperatures=[0, 0.5, 1.0, 1.5, 2], n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bcf56-6824-4061-8d1b-5ac046631908",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab134d6-b509-46d0-b3b6-c21fa30eb4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula estadísticas básicas para cada método\n",
    "statistics = all_results.groupby('temperature').agg({\n",
    "    'TPPI': ['mean', 'std', 'min', 'max'],\n",
    "    'BertScore': ['mean'],\n",
    "    'Perplexity': ['mean'],\n",
    "    'Flesch' : ['mean']\n",
    "}).reset_index()\n",
    "\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc7a4c8-990e-4d47-bd62-4429a4904087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean values for each 'n'\n",
    "mean_values = all_results[['TPPI', 'Normalized_BertScore', 'Normalized_Perplexity', 'Normalized_Flesch', 'temperature']].groupby('temperature').mean().reset_index()\n",
    "# Generating error bars (standard deviation as an example)\n",
    "std_values = all_results[['TPPI', 'Normalized_BertScore', 'Normalized_Perplexity', 'Normalized_Flesch', 'temperature']].groupby('temperature').std().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.errorbar(mean_values['temperature'], mean_values['TPPI'], yerr=std_values['TPPI'], marker='o', label='Mean TPPI', capsize=5)\n",
    "plt.errorbar(mean_values['temperature'], mean_values['Normalized_BertScore'], yerr=std_values['Normalized_BertScore'], marker='o', label='Mean Normalized BertScore', capsize=5)\n",
    "plt.errorbar(mean_values['temperature'], mean_values['Normalized_Perplexity'], yerr=std_values['Normalized_Perplexity'], marker='o', label='Mean Normalized Perplexity', capsize=5)\n",
    "plt.errorbar(mean_values['temperature'], mean_values['Normalized_Flesch'], yerr=std_values['Normalized_Flesch'], marker='o', label='Mean Normalized Flesch', capsize=5)\n",
    "\n",
    "plt.title('TPPI and métricas normalizadas vs. temperatura')\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Puntuación')\n",
    "plt.xticks(mean_values['temperature'])  # Ensure x-axis labels match the temperature values\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
