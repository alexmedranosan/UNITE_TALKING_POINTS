{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7f4ce1-ad19-473c-a76c-2e543fac8344",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "\n",
    "For this type of system we will carry out a joint experimentation. Where we will experiment with:\n",
    "- Text splitting\n",
    "- Vectorisation method\n",
    "\n",
    "To measure the quality of the system, we will use the defined metric TPPI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f76fb2-a457-4e79-970b-a152253ae0b7",
   "metadata": {},
   "source": [
    "## 0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b777189-c8c7-4115-8fdb-36f003c93b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to home project directory\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2239e1-4113-4db0-9e0c-574ad2fb7d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "from src.unite_talking_points.utils.config.config_loader import ConfigLoader\n",
    "from typing import TypedDict\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b911d69-d479-4168-8a82-3125786f817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigLoader().load_config(current_directory_is_root=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb96a3e-5b5e-4a42-a4ab-4c401c321c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = config['External-services']['openai_api_key']\n",
    "client = openai.OpenAI(api_key=config['External-services']['openai_api_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95264f1-3982-4e7b-bf11-bfb3de47aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPPIResult(TypedDict):\n",
    "    TPPI: float\n",
    "    BertScore: float\n",
    "    Normalized_BertScore: float\n",
    "    Perplexity: float\n",
    "    Normalized_Perplexity: float\n",
    "    Flesch: float\n",
    "    Normalized_Flesch: float\n",
    "\n",
    "\n",
    "class TPPI:\n",
    "    def __init__(self, model_type: str = 'bert-base-uncased'):\n",
    "        \"\"\"\n",
    "        Inicializa el normalizador de puntuaciones y los modelos necesarios para los cálculos.\n",
    "        \"\"\"\n",
    "        self.scorer = BERTScorer(model_type=model_type)\n",
    "        self.gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        self.gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_bert_score(bert_score: float) -> float:\n",
    "        \"\"\"\n",
    "        Normaliza el puntaje BERT en un rango de 0 a 1.\n",
    "        \"\"\"\n",
    "        return (bert_score + 1) / 2\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_perplexity(perplexity: float) -> float:\n",
    "        \"\"\"\n",
    "        Normaliza el valor de la perplejidad en un rango de 0 a 1.\n",
    "        \"\"\"\n",
    "        # Aseguramos que la perplexidad mínima sea al menos 1 para evitar división por cero\n",
    "        perplexity = max(perplexity, 1)\n",
    "        # Invertimos la fórmula para que valores bajos de perplexidad den valores altos normalizados\n",
    "        return 1 - (min(perplexity, 100) - 1) / 99\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_flesch(flesch: float) -> float:\n",
    "        \"\"\"\n",
    "        Normaliza el puntaje de Flesch en un rango de 0 a 1.\n",
    "        \"\"\"\n",
    "        return max(min(flesch, 100), 0) / 100\n",
    "\n",
    "    def calculate_bert_score(self, reference_text: str, generated_text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula el BertScore entre un texto de referencia y uno generado.\n",
    "        \"\"\"\n",
    "        P, R, F1 = self.scorer.score([reference_text], [generated_text], verbose=False)\n",
    "        bert_score = F1.item()\n",
    "        return bert_score\n",
    "\n",
    "    def calculate_perplexity(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula la perplejidad de un texto utilizando el modelo GPT-2.\n",
    "        \"\"\"\n",
    "        encode = self.gpt2_tokenizer.encode(text, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            loss = self.gpt2_model(encode, labels=encode)[0]\n",
    "        perplexity = torch.exp(loss).item()\n",
    "        return perplexity\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_flesch(text: str) -> float:\n",
    "        \"\"\"\n",
    "        Calcula el puntaje de Flesch Reading Ease de un texto.\n",
    "        \"\"\"\n",
    "        flesch_score = flesch_reading_ease(text)\n",
    "        return flesch_score\n",
    "\n",
    "    def calculate_tppi(self, original_text: str, generated_text: str) -> TPPIResult:\n",
    "        \"\"\"\n",
    "        Calcula el TPPI basado en los textos de referencia y generados, devolviendo los puntajes de BERT,\n",
    "        perplejidad y Flesch tanto normalizados como sin normalizar.\n",
    "        \"\"\"\n",
    "        # Calcula BertScore\n",
    "        bert_score = self.calculate_bert_score(original_text, generated_text)\n",
    "        normalized_bert_score = self.normalize_bert_score(bert_score)\n",
    "\n",
    "        # Calcula Perplejidad\n",
    "        perplexity = self.calculate_perplexity(generated_text)\n",
    "        normalized_perplexity = self.normalize_perplexity(perplexity)\n",
    "\n",
    "        # Calcula Flesch\n",
    "        flesch = self.calculate_flesch(generated_text)\n",
    "        normalized_flesch = self.normalize_flesch(flesch)\n",
    "\n",
    "        # Calcula TPPI\n",
    "        tppi_score = 0.5 * normalized_bert_score + 0.25 * normalized_perplexity + 0.25 * normalized_flesch\n",
    "\n",
    "        # Devuelve todos los valores en un TypedDict\n",
    "        return TPPIResult(\n",
    "            TPPI=tppi_score,\n",
    "            BertScore=bert_score,\n",
    "            Normalized_BertScore=normalized_bert_score,\n",
    "            Perplexity=perplexity,\n",
    "            Normalized_Perplexity=normalized_perplexity,\n",
    "            Flesch=flesch,\n",
    "            Normalized_Flesch=normalized_flesch\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4228e3b0-2e94-4243-a82b-d96163d0d235",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7714c7b0-9fcb-4b89-818b-0374ac868f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load JSON data from the specified directory into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - data_dir (str): Path to the directory containing JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): DataFrame containing the loaded JSON data.\n",
    "    \"\"\"\n",
    "    # Initialize empty lists to store data\n",
    "    file_names = []\n",
    "    labels = []\n",
    "    document_names = []\n",
    "    meeting_names = []\n",
    "    meeting_dates = []\n",
    "    contents = []\n",
    "    prompts = []\n",
    "\n",
    "    # Iterate over each JSON file in the directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(data_dir, filename), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                # Extract data from each JSON file and append to lists\n",
    "                file_names.append(filename)\n",
    "                labels.append(data['label'])\n",
    "                document_names.append(data['document_name'])\n",
    "                meeting_names.append(data['meeting_name'])\n",
    "                meeting_dates.append(data['meeting_date'])\n",
    "                contents.append(data['content'])\n",
    "                prompts.append(data['prompt'])\n",
    "\n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        'file_name': file_names,\n",
    "        'label': labels,\n",
    "        'document_name': document_names,\n",
    "        'meeting_name': meeting_names,\n",
    "        'meeting_date': meeting_dates,\n",
    "        'content': contents,\n",
    "        'prompt': prompts\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62d467-df98-445e-90dd-2e162c20fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_json_data(config['Directories']['raw_data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e176ea4-1ea7-4f90-b12b-e987085eddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dabcf0-af52-4ba6-94b7-c05a323b8fb1",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1a6db6-f7dc-488b-9cbe-2d178dde328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8c6f84-f7aa-4f87-9071-714b377c6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Parse the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization, removing stop words, punctuation, and lemmatization\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1789f2a5-7097-4e52-8dfb-66fedf7641b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'content' column\n",
    "df['preprocessed_content'] = df['content'].apply(preprocess_text)\n",
    "df['preprocessed_prompt'] = df['prompt'].apply(preprocess_text)\n",
    "\n",
    "# Display the preprocessed content\n",
    "print(\"Preprocessed Content:\")\n",
    "print(df['preprocessed_content'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a004fb5-534f-4fe9-b473-1667d09a94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meeting_date'] = pd.to_datetime(df['meeting_date'], format='%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb42ed-05ef-497a-b798-f2f2da339560",
   "metadata": {},
   "source": [
    "## 3. Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c1596-74c6-4791-8105-3a69d916f676",
   "metadata": {},
   "source": [
    "### 3.1 Vectorization experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea52a6b-9a05-4db5-8e11-6af2af7bf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_embedding(text, model=\"text-embedding-3-large\"):\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=[text],\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    # Extract the embedding vector\n",
    "    embedding = np.array(response.data[0].embedding).reshape(1, -1)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3239a-5785-4d24-89fa-ce5339551ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorización con TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['preprocessed_content'])\n",
    "tfidf_prompt_matrix = tfidf_vectorizer.transform(df['preprocessed_prompt'])\n",
    "\n",
    "# Vectorización con OpenAI\n",
    "embedding_matrix = np.vstack(df['content'].apply(lambda x: get_openai_embedding(x)))\n",
    "embedding_prompt_matrix = np.vstack(df['prompt'].apply(lambda x: get_openai_embedding(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95aef08a-f06d-4a02-826d-79a2a3e52ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TPPI scoring system\n",
    "tppi = TPPI()\n",
    "\n",
    "# Function to conduct experiments\n",
    "def conduct_experiment(df, vector_matrix, prompt_matrix, method):\n",
    "    results = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        # Find the most relevant document\n",
    "        cosine_similarities = cosine_similarity(prompt_matrix[i].reshape(1, -1), vector_matrix).flatten()\n",
    "        cosine_similarities[i] = -1  # exclude the document itself\n",
    "        most_relevant_doc_index = cosine_similarities.argmax()\n",
    "        most_relevant_doc = df.iloc[most_relevant_doc_index]\n",
    "\n",
    "        # Calculate TPPI\n",
    "        tppi_result = tppi.calculate_tppi(df['content'][i], most_relevant_doc['content'])\n",
    "        tppi_result['file_name'] = df['file_name'][i]\n",
    "        tppi_result['method'] = method\n",
    "        results.append(tppi_result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# Run experiments\n",
    "tfidf_results = conduct_experiment(df, tfidf_matrix, tfidf_prompt_matrix, \"TF-IDF\")\n",
    "embedding_results = conduct_experiment(df, embedding_matrix, embedding_prompt_matrix, \"Embedding\")\n",
    "\n",
    "# Combine and save results\n",
    "all_results = pd.DataFrame(tfidf_results + embedding_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6d63d3-9f54-4c9d-a888-d209aa865b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula estadísticas básicas para cada método\n",
    "statistics = all_results.groupby('method').agg({\n",
    "    'TPPI': ['mean', 'std', 'min', 'max'],\n",
    "    'BertScore': ['mean'],\n",
    "    'Perplexity': ['mean'],\n",
    "    'Flesch' : ['mean']\n",
    "}).reset_index()\n",
    "\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fe270a-ce02-4329-badc-dec071fb975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los nombres de las métricas para iterar\n",
    "metrics = ['TPPI', 'Normalized_BertScore', 'Normalized_Perplexity', 'Normalized_Flesch']\n",
    "\n",
    "# Crear un subplot 2x2 para los boxplots de cada métrica\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "fig.suptitle('Distribución de Métricas por Método de Vectorización', fontsize=16)\n",
    "\n",
    "# Iterar sobre las métricas y los ejes para crear los boxplots\n",
    "for ax, metric in zip(axes.flatten(), metrics):\n",
    "    all_results.boxplot(column=metric, by='method', ax=ax, grid=False)\n",
    "    ax.set_title(metric)\n",
    "    ax.set_xlabel('Método de Vectorización')\n",
    "    ax.set_ylabel('Puntuación')\n",
    "    ax.set_ylim(0, 1)  # Asegurar que el gráfico esté limitado entre 0 y 1\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)  # Rotar las etiquetas para mejor legibilidad\n",
    "\n",
    "# Ajustar el layout para evitar la superposición de los títulos y etiquetas\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])  # Ajusta los límites del rectángulo del layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ae292-03de-4abf-80c8-abc1faeb62b0",
   "metadata": {},
   "source": [
    "### 3.2 Chuking experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422fab78-e7a4-49ff-95e4-1bea541bad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_by_newline(df):\n",
    "    # Crear una lista de tuplas (índice original, frase)\n",
    "    tuples = []\n",
    "    for index, row in df.iterrows():\n",
    "        for sentence in row['content'].split('\\n'):\n",
    "            tuples.append((index, row['file_name'], sentence.strip()))\n",
    "\n",
    "    # Convertir la lista de tuplas en un DataFrame\n",
    "    new_df = pd.DataFrame(tuples, columns=['original_index', 'file_name', 'sentence'])\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49ed53-fdd1-419d-ba4c-c508139dec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = expand_by_newline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e298b7a-23b3-4893-9e6f-26416cdae6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_sentences_matrix = np.vstack(sentences['sentence'].apply(lambda x: get_openai_embedding(x)))\n",
    "embedding_prompt_matrix = np.vstack(df['prompt'].apply(lambda x: get_openai_embedding(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5672c019-d8df-4fa8-b774-e576d3ca94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conduct_experiment(df, sentences, vector_matrix, prompt_matrix, n=5):\n",
    "    results = []\n",
    "    for i in tqdm(range(len(df))):\n",
    "        # Find the most relevant document\n",
    "        cosine_similarities = cosine_similarity(prompt_matrix[i].reshape(1, -1), vector_matrix).flatten()\n",
    "        cosine_similarities[sentences.original_index == i] = -1  # exclude the document itself\n",
    "        top_n_indices = cosine_similarities.argsort()[-n:][::-1]\n",
    "        combined_content = '. '.join(sentences['sentence'][j] for j in top_n_indices)\n",
    "\n",
    "        # Calculate TPPI\n",
    "        tppi_result = tppi.calculate_tppi(df['content'][i], combined_content)\n",
    "        tppi_result['file_name'] = df['file_name'][i]\n",
    "        tppi_result['n'] = n\n",
    "        results.append(tppi_result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30750527-25e6-4de9-958a-c6677c63a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "for i in range(1, 11):\n",
    "    all_results += conduct_experiment(df, sentences, embedding_sentences_matrix, embedding_prompt_matrix, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ee3457-d04e-4a14-bca5-36b559faf6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ebe8e9-d16f-4ae1-8e0f-30ba13df50ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula estadísticas básicas para cada método\n",
    "statistics = all_results.groupby('n').agg({\n",
    "    'TPPI': ['mean', 'std', 'min', 'max'],\n",
    "    'BertScore': ['mean'],\n",
    "    'Perplexity': ['mean'],\n",
    "    'Flesch' : ['mean']\n",
    "}).reset_index()\n",
    "\n",
    "print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f285a79a-6714-46e7-8a60-85d005d2dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating mean values for each 'n'\n",
    "mean_values = all_results[['TPPI', 'Normalized_BertScore', 'Normalized_Perplexity', 'Normalized_Flesch', 'n']].groupby('n').mean().reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(mean_values['n'], mean_values['TPPI'], marker='o', label='Mean TPPI')\n",
    "plt.plot(mean_values['n'], mean_values['Normalized_BertScore'], marker='o', label='Mean Normalized BertScore')\n",
    "plt.plot(mean_values['n'], mean_values['Normalized_Perplexity'], marker='o', label='Mean Normalized Perplexity')\n",
    "plt.plot(mean_values['n'], mean_values['Normalized_Flesch'], marker='o', label='Mean Normalized Flesch')\n",
    "\n",
    "plt.title('Mean Evolution of TPPI and Normalized Scores by n')\n",
    "plt.xlabel('Number of Combined Documents (n)')\n",
    "plt.ylabel('Mean Score')\n",
    "plt.xticks(mean_values['n'])  # Ensure x-axis labels match the number of documents\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
