{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f04f66c-7ad7-4764-bb40-b00ca18b2bab",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba1349b-7c2f-401a-95a6-fab2f3b32885",
   "metadata": {},
   "source": [
    "## 0. Environment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06abe97e-59eb-41dd-9f05-8b35bf7d9a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to home project directory\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfb628-cd5d-472e-9d59-031a08c07d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "from bert_score import BERTScorer\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from textstat import flesch_reading_ease\n",
    "\n",
    "from src.unite_talking_points.utils.config.config_loader import ConfigLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361256a0-01d6-43dc-b58a-0cf30dde08d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigLoader().load_config(current_directory_is_root=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015ceb01-94a0-4b36-9ca0-eb29ebbd7230",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93f2d8-9e1d-47ad-b498-e50f15608f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load JSON data from the specified directory into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - data_dir (str): Path to the directory containing JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): DataFrame containing the loaded JSON data.\n",
    "    \"\"\"\n",
    "    # Initialize empty lists to store data\n",
    "    file_names = []\n",
    "    labels = []\n",
    "    document_names = []\n",
    "    meeting_names = []\n",
    "    meeting_dates = []\n",
    "    contents = []\n",
    "    prompts = []\n",
    "\n",
    "    # Iterate over each JSON file in the directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(data_dir, filename), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                # Extract data from each JSON file and append to lists\n",
    "                file_names.append(filename)\n",
    "                labels.append(data['label'])\n",
    "                document_names.append(data['document_name'])\n",
    "                meeting_names.append(data['meeting_name'])\n",
    "                meeting_dates.append(data['meeting_date'])\n",
    "                contents.append(data['content'])\n",
    "                prompts.append(data['prompt'])\n",
    "\n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        'file_name': file_names,\n",
    "        'label': labels,\n",
    "        'document_name': document_names,\n",
    "        'meeting_name': meeting_names,\n",
    "        'meeting_date': meeting_dates,\n",
    "        'content': contents,\n",
    "        'prompt': prompts\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0285855e-a850-4a84-898f-f845e0eac18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_json_data(config['Directories']['raw_data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbddad19-58c8-4f6b-a026-51df3ed523cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cae48-4f31-4f85-8f29-349447fea5ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Basics statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76537d-2cfd-4c87-98a1-c71127c222b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of documents\n",
    "num_documents = len(df)\n",
    "\n",
    "# Distribution of labels\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Distribution of meeting names\n",
    "meeting_counts = df['meeting_name'].value_counts()\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Basic Statistics:\")\n",
    "print(\"Number of documents:\", num_documents)\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(label_counts)\n",
    "print(\"\\nMeeting Name Distribution:\")\n",
    "print(meeting_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1e4c98-b4ed-4cec-9be6-2f5c96f2a589",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e20db5e-0e1e-472a-b03b-da20e2609a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_trf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad205ee-49de-4fac-b5ed-4f9143a92ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Parse the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenization, removing stop words, punctuation, and lemmatization\n",
    "    tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    \n",
    "    # Join the tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d009ac-1b6b-415c-bd6a-89f4a66d25da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the preprocessing function to the 'content' column\n",
    "df['preprocessed_content'] = df['content'].apply(preprocess_text)\n",
    "\n",
    "# Display the preprocessed content\n",
    "print(\"Preprocessed Content:\")\n",
    "print(df['preprocessed_content'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6ed74e-9385-4670-be35-66f36979cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meeting_date'] = pd.to_datetime(df['meeting_date'], format='%d-%m-%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a3736-5645-436b-ad33-0cd261871b74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a7743e-adc7-454b-b2aa-53919fdc0d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate word frequencies\n",
    "def calculate_word_frequencies(text):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    \n",
    "    # Create a pandas Series from the list of words\n",
    "    word_series = pd.Series(words)\n",
    "    \n",
    "    # Count the occurrences of each word\n",
    "    word_counts = word_series.value_counts()\n",
    "\n",
    "    return word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f4e9f-4036-4624-8748-f18af49c48e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate preprocessed content for all labels\n",
    "all_concatenated_text = ' '.join(df['preprocessed_content'])\n",
    "\n",
    "# Calculate word frequencies for all labels\n",
    "all_word_frequencies = calculate_word_frequencies(all_concatenated_text)\n",
    "\n",
    "# Create a figure and axes for subplots\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 6))\n",
    "\n",
    "# Plot word frequency analysis for all labels using Seaborn\n",
    "sns.barplot(x=all_word_frequencies.head(20).values, y=all_word_frequencies.head(20).index, ax=axes)\n",
    "axes.set_title('An√°lisis frecuencial de palabras global')\n",
    "axes.set_xlabel('Frecuencia')\n",
    "axes.set_ylabel('Palabra')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('img/frecuencia_global.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a609466-c6c2-46db-830d-c6bb8be1547e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word frequencies for each label\n",
    "word_frequencies_by_label = {}\n",
    "for label in df['label'].unique():\n",
    "    # Filter the DataFrame by label\n",
    "    subset_df = df[df['label'] == label]\n",
    "    \n",
    "    # Concatenate preprocessed content of all documents for the label\n",
    "    concatenated_text = ' '.join(subset_df['preprocessed_content'])\n",
    "    \n",
    "    # Calculate word frequencies\n",
    "    word_frequencies_by_label[label] = calculate_word_frequencies(concatenated_text)\n",
    "\n",
    "# Create a figure and axes for subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Flatten the axes array to facilitate iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot word frequency analysis for each label using Seaborn\n",
    "n = 10\n",
    "for i, (label, word_freq) in enumerate(word_frequencies_by_label.items()):\n",
    "    # Plot on the appropriate subplot\n",
    "    sns.barplot(x=word_freq.head(n).values, y=word_freq.head(n).index, ax=axes[i])\n",
    "    axes[i].set_title(f'{label}')\n",
    "    axes[i].set_xlabel('Palabra')\n",
    "    axes[i].set_ylabel('Frecuencia')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(len(word_frequencies_by_label), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle('An√°lisis frecuencial de palabras por etiquetas tem√°ticas', fontsize=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig('img/frecuencia_local.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95556ee-1d2e-4a9f-b857-554775ac712c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. N-gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8176f83-5361-45ee-8289-b85f691cbc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate n-gram frequencies\n",
    "def calculate_ngram_frequencies(text, n):\n",
    "    # Initialize CountVectorizer to extract n-grams\n",
    "    vectorizer = CountVectorizer(ngram_range=(n, n), stop_words='english')\n",
    "    \n",
    "    # Fit and transform the text to extract n-grams\n",
    "    ngrams = vectorizer.fit_transform([text])\n",
    "    \n",
    "    # Get the feature names (n-grams)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get the counts of each n-gram\n",
    "    ngram_counts = ngrams.toarray().flatten()\n",
    "    \n",
    "    # Create a pandas Series from the counts with n-gram names as index\n",
    "    ngram_freq = pd.Series(ngram_counts, index=feature_names).sort_values(ascending=False)\n",
    "    \n",
    "    return ngram_freq\n",
    "\n",
    "# Function to plot n-gram analysis\n",
    "def plot_ngram_analysis(ngram_freq, n, label=''):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ngram_freq.head(20).plot(kind='barh')\n",
    "    if label:\n",
    "        plt.title(f'{label}')\n",
    "    else:\n",
    "        plt.title(f'An√°lisis frecuencial global de bigramas')\n",
    "    plt.xlabel(f'Frecuencia')\n",
    "    plt.ylabel('Bigrama')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('img/bigramas_global.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48cc988-b12b-4be0-a69a-3a93f661a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_text = ' '.join(df['preprocessed_content'])\n",
    "ngram_freq = calculate_ngram_frequencies(concatenated_text, n=2)\n",
    "\n",
    "# Plot N-gram analysis\n",
    "plot_ngram_analysis(ngram_freq, n=2)\n",
    "plt.savefig('bigramas_global.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4865d-45c0-40f8-8170-5b2c2bc24a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform N-gram analysis for each label\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Flatten the axes array to facilitate iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, label in enumerate(df['label'].unique()):\n",
    "    # Filter the DataFrame by label\n",
    "    subset_df = df[df['label'] == label]\n",
    "    \n",
    "    # Concatenate preprocessed content of all documents for the label\n",
    "    concatenated_text = ' '.join(subset_df['preprocessed_content'])\n",
    "    \n",
    "    # Calculate N-gram frequencies for bi-grams (n=2)\n",
    "    ngram_freq = calculate_ngram_frequencies(concatenated_text, n=2)\n",
    "    \n",
    "    # Plot N-gram analysis with rotated orientation on the appropriate subplot\n",
    "    ax = axes[idx]\n",
    "    ngram_freq.head(10).sort_values().plot(kind='barh', ax=ax)\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.set_xlabel('Frecuencia')\n",
    "    ax.set_ylabel('Bigrama')\n",
    "    ax.invert_yaxis()  # Invert y-axis to have highest frequency at the top\n",
    "\n",
    "# Hide empty subplots\n",
    "for j in range(len(df['label'].unique()), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.suptitle('An√°lisis frecuencial de bigramas por etiquetas tem√°ticas', fontsize=24)\n",
    "plt.tight_layout()\n",
    "plt.savefig('img/bigrama_local.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d195b-73c8-48f1-91a5-e0a2885c6dba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Named Entity Recognition Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef54d14-3669-4784-b942-74ad5a112bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_ner_with_entities(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Initialize defaultdict to count entity occurrences\n",
    "    entity_counts = defaultdict(int)\n",
    "    \n",
    "    # List to store unique entities\n",
    "    unique_entities = set()\n",
    "    \n",
    "    # Iterate over entities in the document\n",
    "    for ent in doc.ents:\n",
    "        # Count occurrences of each entity label\n",
    "        entity_counts[ent.label_] += 1\n",
    "        \n",
    "        # Add entity text to unique_entities set\n",
    "        unique_entities.add((ent.text, ent.label_))\n",
    "    \n",
    "    return entity_counts, unique_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1990218-755d-4465-9a2a-c13b9ff28e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform NER and extract unique entities for each label\n",
    "ner_results_by_label = {}\n",
    "unique_entities_by_label = {}\n",
    "for label in df['label'].unique():\n",
    "    # Filter the DataFrame by label\n",
    "    subset_df = df[df['label'] == label]\n",
    "    \n",
    "    # Concatenate preprocessed content of all documents for the label\n",
    "    concatenated_text = ' '.join(subset_df['preprocessed_content'])\n",
    "    \n",
    "    # Perform NER and extract unique entities\n",
    "    ner_results, unique_entities = perform_ner_with_entities(concatenated_text)\n",
    "    ner_results_by_label[label] = ner_results\n",
    "    unique_entities_by_label[label] = unique_entities\n",
    "\n",
    "# Display NER results and unique entities for each label\n",
    "for label, ner_results in ner_results_by_label.items():\n",
    "    #print(f\"NER Results for Label: {label}\")\n",
    "    #for ent_label, count in ner_results.items():\n",
    "    #    print(f\"{ent_label}: {count}\")\n",
    "    #print()\n",
    "    print(f\"Unique Entities for Label: {label}\")\n",
    "    for entity, ent_label in unique_entities_by_label[label]:\n",
    "        print(f\"{entity} ({ent_label})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfe3a3c-4f99-4ea3-b190-b3c59fd5310a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7. Document Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52812c27-5955-48d5-b8ec-71b033c305c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate document lengths (number of words)\n",
    "df['document_length'] = df['content'].apply(lambda x: len(x.split()))\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "sns.kdeplot(data=df, x=\"document_length\", ax=ax1)\n",
    "\n",
    "ax1.set_xlim((df[\"document_length\"].min(), df[\"document_length\"].max()))\n",
    "ax1.set_xlabel('N√∫mero de palabras')\n",
    "ax1.set_ylabel('Densidad')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('N√∫mero de documentos')\n",
    "sns.histplot(data=df, x=\"document_length\", bins=15, ax=ax2)\n",
    "plt.title('Distribuci√≥n de la longitud de documentos')\n",
    "plt.savefig('img/distribucion_longitud.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a4e9e6-f32a-4c65-a61d-7800f64f5afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot text length vs. label analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=df, x='label', y='document_length')\n",
    "plt.title('Distribuci√≥n de la longitud por etiqueta tem√°tica')\n",
    "plt.xlabel('Etiqueta tem√°tica')\n",
    "plt.ylabel('N√∫mero de palabras')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig('img/distribucion_longitud_etiquetas.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82efd382-2f5f-4e8e-9295-a2fbee58a51f",
   "metadata": {},
   "source": [
    "## 8. Text Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbe008-25c5-4b86-888a-ff741be63025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit-transform the preprocessed content to create TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['preprocessed_content'])\n",
    "\n",
    "# Compute pairwise cosine similarities between documents\n",
    "tfidf_cosine_similarities = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Plot heatmap of cosine similarities\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(tfidf_cosine_similarities, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "plt.title('Similaridad coseno entre documentos', fontsize=18)\n",
    "plt.xlabel('ID del documento')\n",
    "plt.ylabel('ID del documento')\n",
    "plt.savefig('img/similaridad_coseno.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f812910f-7d50-4b14-98eb-4c448d848997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√°scara para excluir la diagonal principal (similaridad de cada documento consigo mismo)\n",
    "mask = np.ones(tfidf_cosine_similarities.shape, dtype=bool)\n",
    "np.fill_diagonal(mask, 0)\n",
    "\n",
    "# Calcula la media de las similitudes de coseno excluyendo la diagonal\n",
    "mean_cosine_similarity = np.mean(tfidf_cosine_similarities[mask])\n",
    "\n",
    "print(\"La media de las similaridades de coseno (excluyendo diagonal) es:\", mean_cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1fcc1c-485a-4b29-99f3-11ff01dfc91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique labels\n",
    "unique_labels = df['label'].unique()\n",
    "\n",
    "# Create subplots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))  # Adjusted figsize for better fit\n",
    "\n",
    "# Flatten the axes array to facilitate iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, label in enumerate(unique_labels):\n",
    "    # Filter the DataFrame by label\n",
    "    subset_df = df[df['label'] == label]\n",
    "    \n",
    "    # Calculate cosine similarities within label subset\n",
    "    tfidf_matrix_label = tfidf_vectorizer.transform(subset_df['preprocessed_content'])\n",
    "    tfidf_cosine_similarities_label = cosine_similarity(tfidf_matrix_label, tfidf_matrix_label)\n",
    "    \n",
    "    # Plot heatmap\n",
    "    ax = axes[idx]\n",
    "    sns.heatmap(tfidf_cosine_similarities_label, ax=ax, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "    ax.set_title(f'{label}')\n",
    "    ax.set_xlabel('ID del documento')\n",
    "    ax.set_ylabel('ID del documento')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust the rectangle in tight_layout\n",
    "plt.suptitle('Similaridad coseno entre documentos por etiqueta tem√°tica', fontsize=24, y=0.98)  # Adjust the y position of suptitle\n",
    "plt.savefig('img/similaridad_coseno_label.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe71627-ec09-4b78-800f-ab9bd0c10de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa una lista para guardar las medias de las similitudes de coseno para cada etiqueta\n",
    "mean_cosine_similarities_per_label = []\n",
    "\n",
    "# Itera sobre cada etiqueta √∫nica en el DataFrame\n",
    "for label in df['label'].unique():\n",
    "    # Filtra el DataFrame por etiqueta\n",
    "    subset_df = df[df['label'] == label]\n",
    "    \n",
    "    # Calcula las similitudes de coseno dentro del subconjunto de la etiqueta\n",
    "    tfidf_matrix_label = tfidf_vectorizer.transform(subset_df['preprocessed_content'])\n",
    "    tfidf_cosine_similarities_label = cosine_similarity(tfidf_matrix_label, tfidf_matrix_label)\n",
    "    \n",
    "    # M√°scara para excluir la diagonal principal\n",
    "    mask = np.ones(tfidf_cosine_similarities_label.shape, dtype=bool)\n",
    "    np.fill_diagonal(mask, 0)\n",
    "    \n",
    "    # Calcula la media de las similitudes de coseno excluyendo la diagonal\n",
    "    mean_cosine_similarity = np.mean(tfidf_cosine_similarities_label[mask])\n",
    "    mean_cosine_similarities_per_label.append(mean_cosine_similarity)\n",
    "\n",
    "# Calcula la media de las medias de las similitudes de coseno\n",
    "overall_mean_cosine_similarity = np.mean(mean_cosine_similarities_per_label)\n",
    "\n",
    "# Imprime las medias de similaridades de coseno para cada etiqueta y la media general\n",
    "for idx, label in enumerate(df['label'].unique()):\n",
    "    print(f\"Media de similaridad de coseno para la etiqueta '{label}': {mean_cosine_similarities_per_label[idx]}\")\n",
    "print(f\"Media general de las medias de similaridades de coseno: {overall_mean_cosine_similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7370348a-a57a-4e18-81a8-41750a00eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "bert_scores = np.zeros((len(df), len(df)))\n",
    "\n",
    "for i in tqdm(range(len(df))):\n",
    "    for j in tqdm(range(i, len(df)), leave=False):  # Compute half the matrix due to symmetry\n",
    "        P, R, F1 = scorer.score([df['content'][i]], [df['content'][j]], verbose=False)\n",
    "        bert_scores[i, j] = F1.item()\n",
    "        bert_scores[j, i] = F1.item()  # Fill both (i, j) and (j, i)\n",
    "\n",
    "# Plot heatmap of BERTScore similarities\n",
    "plt.figure(figsize=(20, 8))\n",
    "sns.heatmap(bert_scores, cmap='coolwarm', annot=True, fmt=\".2f\")\n",
    "plt.title('BERTScore entre documentos', fontsize=18)\n",
    "plt.xlabel('ID del documento')\n",
    "plt.ylabel('ID del documento')\n",
    "plt.savefig('img/bert_score.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa73cf0-611c-4ba5-9521-6b4cbce6a150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√°scara para excluir la diagonal principal (similaridad de cada documento consigo mismo)\n",
    "mask = np.ones(bert_scores.shape, dtype=bool)\n",
    "np.fill_diagonal(mask, 0)\n",
    "\n",
    "# Calcula la media de las similitudes bertscore excluyendo la diagonal\n",
    "mean_bertscore_similarity = np.mean(bert_scores[mask])\n",
    "\n",
    "print(\"La media de las similaridades de coseno (excluyendo diagonal) es:\", mean_bertscore_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794b7ca-99c5-4955-a302-25d3052cec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para extraer la parte triangular superior de la matriz sin la diagonal\n",
    "def get_upper_triangle(matrix):\n",
    "    return matrix[np.triu_indices_from(matrix, k=1)]\n",
    "\n",
    "# Aplanar las matrices\n",
    "flat_bert_scores = get_upper_triangle(bert_scores)\n",
    "flat_cosine_distances = get_upper_triangle(tfidf_cosine_similarities)\n",
    "\n",
    "# Calcular correlaciones\n",
    "pearson_corr, _ = pearsonr(flat_bert_scores, flat_cosine_distances)\n",
    "spearman_corr, _ = spearmanr(flat_bert_scores, flat_cosine_distances)\n",
    "kendall_corr, _ = kendalltau(flat_bert_scores, flat_cosine_distances)\n",
    "\n",
    "print(\"Correlaci√≥n de Pearson:\", pearson_corr)\n",
    "print(\"Correlaci√≥n de Spearman:\", spearman_corr)\n",
    "print(\"Correlaci√≥n de Kendall:\", kendall_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97fb18-ee5f-4c62-ac2e-058f228149d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9. Prompt Similarity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af75e41e-6652-4177-ae41-af5b93b37e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccards = []\n",
    "for doc, prompt in zip(df['preprocessed_content'], df['prompt']):\n",
    "    doc_vector = tfidf_vectorizer.transform([doc])\n",
    "    prompt_vector = tfidf_vectorizer.transform([prompt])\n",
    "    jaccards.append(cosine_similarity(doc_vector, prompt_vector)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d43d9-d65c-4e6b-80f5-358f00739d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "sns.kdeplot(jaccards, ax=ax1)\n",
    "ax1.set_xlim((min(jaccards), max(jaccards)))\n",
    "ax2 = ax1.twinx()\n",
    "sns.histplot(jaccards, bins=15, ax=ax2)\n",
    "plt.title('Content-Prompt Jaccard similarity distribution')\n",
    "plt.xlabel('Jaccard similarity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c121ce7a-c8f6-4146-bb15-0db15480c989",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_scores = []\n",
    "for doc, prompt in tqdm(zip(df['content'], df['prompt']), total=len(df)):\n",
    "    P, R, F1 = scorer.score([doc], [prompt], verbose=False)\n",
    "    bert_scores.append(F1.data.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2102d94e-2b0d-432e-a33d-54e6184be601",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(bert_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7e322-7ae7-4057-be3e-ad8e1e4c559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "sns.kdeplot(bert_scores, ax=ax1)\n",
    "ax1.set_xlim((min(bert_scores), max(bert_scores)))\n",
    "ax1.set_xlabel('N√∫mero de palabras')\n",
    "ax1.set_ylabel('Densidad')\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('N√∫mero de documentos')\n",
    "sns.histplot(bert_scores, bins=15, ax=ax2)\n",
    "plt.title('Disitribuci√≥n de la similaridad entre contenido y prompt.')\n",
    "plt.savefig('img/bertscores_prompt_content.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8723da-f1c9-4f3c-b5c6-2e075fc2c0c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 10. Fluidity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ce1a7e-4b2b-484b-b449-b5cd65f78552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Function to calculate perplexity\n",
    "def calculate_perplexity(text):\n",
    "    encode = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        loss = model(encode, labels=encode)[0]\n",
    "    return torch.exp(loss).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f07dc-5d7b-44bd-b4ea-f3ab2abf3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexities = df['content'].apply(lambda x: calculate_perplexity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab24e09-45b8-4a2f-bda8-7d393c55b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afb7ff-9cfb-4c5a-ac22-ddd9c59dcdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "sns.kdeplot(perplexities, ax=ax1)\n",
    "ax1.set_xlim((min(perplexities), max(perplexities)))\n",
    "ax1.set_xlabel('Perplejidad de GPT-2')\n",
    "ax1.set_ylabel('Densidad')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('N√∫mero de documentos')\n",
    "sns.histplot(perplexities, bins=15, ax=ax2)\n",
    "plt.title('Distribuci√≥n de la perplejidad de los documentos')\n",
    "plt.savefig('img/perplexity.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82588e02-ee9d-4ce9-9705-964b35570c5e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 11. Readibility Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd0bcde-3bac-443b-b70b-a3be734b71e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fleshs = df['content'].apply(lambda x: flesch_reading_ease(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88406782-839a-4035-afb5-e354585fcf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(fleshs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318e51e-906a-46df-a542-5fe3a37458ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "sns.kdeplot(fleshs, ax=ax1)\n",
    "ax1.set_xlim((min(fleshs), max(fleshs)))\n",
    "ax1.set_xlabel('Puntuaci√≥n')\n",
    "ax1.set_ylabel('Densidad')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('N√∫mero de documentos')\n",
    "sns.histplot(fleshs, bins=15, ax=ax2)\n",
    "plt.title('Flesch Reading Ease de los documentos')\n",
    "plt.xlabel('Flesh score')\n",
    "plt.savefig('img/flesch.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfb5bf2-beab-4a2b-b7d5-b2e81e41bbf0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 12. Date Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7344aaab-3858-4d45-b31e-609340a1015e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['meeting_date'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4220e7-e7f6-4fb4-aa5a-0ce5f81aa098",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "df['meeting_date'].hist(bins=50, alpha=0.7)\n",
    "plt.title('Distribuci√≥n de los documentos a trav√©s del tiempo')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.savefig('img/fechas.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39fffbd-d8d8-4ad1-ab16-ba63c48225c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting month and year\n",
    "df['year'] = df['meeting_date'].dt.year\n",
    "df['month'] = df['meeting_date'].dt.month\n",
    "\n",
    "# Grouping by year and month\n",
    "monthly_counts = df.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "monthly_counts.plot(kind='bar', stacked=True)\n",
    "plt.title('Frequency of Documents by Month and Year')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(title='Month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eda008-709c-45a6-a5a0-ddfa946dcd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping by date and label\n",
    "label_distribution = df.groupby([df['meeting_date'].dt.to_period('M'), 'label']).size().unstack(fill_value=0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "label_distribution.plot(kind='line', stacked=False)\n",
    "plt.title('Distribution of Labels Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend(title='Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8aecdc-ddc3-4d5c-8b81-f81deaba4a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[19]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0523c5dc-5670-43ec-9a13-0affaa56aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[25].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e51ee2-6a5b-4e8d-bd5e-7753edd6d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[25].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de81f48-dab6-4461-97b8-fae682f01917",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb63cb58-18ee-4762-a951-93833ec1015d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.iloc[22].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
