{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3db4e01b-92b1-4180-a27c-bd2eb3126bea",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d85f61-ad4f-4afd-abd9-052f799b37f1",
   "metadata": {},
   "source": [
    "## 0. Environment set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01216a31-4cf6-405d-b330-56da73df58ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to home project directory\n",
    "%cd ../.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10981e87-b9ba-4645-8ca4-229347c3dde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain import hub\n",
    "\n",
    "import faiss\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import TypedDict\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.unite_talking_points.utils.config.config_loader import ConfigLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a4df0-195f-4d50-abfb-ecc90cfcce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ConfigLoader().load_config(current_directory_is_root=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cb94ea-753d-40a2-b760-d55d8ea13052",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6f3f1b-a1df-4e4f-a79a-7b72fe19a553",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = config['External-services']['openai_api_key']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84e780d-88c9-4396-adee-183ba4aadcf8",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3f19c-8ce7-4118-b4ef-45781aa7c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(data_dir):\n",
    "    \"\"\"\n",
    "    Load JSON data from the specified directory into a pandas DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - data_dir (str): Path to the directory containing JSON files.\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): DataFrame containing the loaded JSON data.\n",
    "    \"\"\"\n",
    "    # Initialize empty lists to store data\n",
    "    file_names = []\n",
    "    labels = []\n",
    "    document_names = []\n",
    "    meeting_names = []\n",
    "    meeting_dates = []\n",
    "    contents = []\n",
    "    prompts = []\n",
    "\n",
    "    # Iterate over each JSON file in the directory\n",
    "    for filename in os.listdir(data_dir):\n",
    "        if filename.endswith('.json'):\n",
    "            with open(os.path.join(data_dir, filename), 'r') as file:\n",
    "                data = json.load(file)\n",
    "                # Extract data from each JSON file and append to lists\n",
    "                file_names.append(filename)\n",
    "                labels.append(data['label'])\n",
    "                document_names.append(data['document_name'])\n",
    "                meeting_names.append(data['meeting_name'])\n",
    "                meeting_dates.append(data['meeting_date'])\n",
    "                contents.append(data['content'])\n",
    "                prompts.append(data['prompt'])\n",
    "\n",
    "    # Create a DataFrame from the lists\n",
    "    df = pd.DataFrame({\n",
    "        'file_name': file_names,\n",
    "        'label': labels,\n",
    "        'document_name': document_names,\n",
    "        'meeting_name': meeting_names,\n",
    "        'meeting_date': meeting_dates,\n",
    "        'content': contents,\n",
    "        'prompt': prompts\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06603530-bb02-4303-b007-ad73500c8e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_json_data(config['Directories']['raw_data_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fc47b-01db-44d2-9aca-3042d2a0b185",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6eb99c-4e8f-4c81-9198-7aad95a5ee57",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae882a4-8b3d-44f9-bfb7-77bd636cc106",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents(df['content'])\n",
    "splits = text_splitter.split_documents(texts)\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectorstore = FAISS.from_documents(texts, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed52329f-8f48-4032-bfd3-51322da511d5",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ea5a6c-df2f-4ef0-950a-f6cf0325f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b65f0-74d1-4c23-9d55-153c3d27f4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "        You are an assistant specialized in creating Talking Points for United Nations. \n",
    "        Use the given context to create a the Talking Point.\n",
    "        The output should contain bullet points on each Talking Point.\n",
    "        If the context does not contain enough information, indicate that clearly.\n",
    "        \n",
    "        Question: {question}\n",
    "        Context: {context}\n",
    "        Answer:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9848ec0-349b-4a9c-940c-46ac8862f0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47ddda-0e80-44f7-ad20-d0f8f2e5d502",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | custom_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "answer = rag_chain.invoke(df.iloc[0].prompt)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffad7b-8f89-41bf-bd8e-3fe3808368ad",
   "metadata": {},
   "source": [
    "## 4. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40349b-a1a6-4063-9cc4-763e4f40e71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_langchain(contents, llm_model=\"gpt-4o-mini\"):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    texts = text_splitter.create_documents(contents)\n",
    "    splits = text_splitter.split_documents(texts)\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vectorstore = FAISS.from_documents(texts, embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    custom_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\n",
    "            You are an assistant specialized in creating Talking Points for United Nations. \n",
    "            Use the given context to create a the Talking Point.\n",
    "            The output should contain bullet points on each Talking Point.\n",
    "            If the context does not contain enough information, indicate that clearly.\n",
    "            \n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model_name=llm_model)\n",
    "\n",
    "    chain = rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff2bf11-0df4-4edf-a3ef-144bee409bd9",
   "metadata": {},
   "source": [
    "### 4.1 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b863cb-bf4e-4f8c-a235-9558143cb9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to conduct n experiments without averaging results\n",
    "def conduct_experiment(df, model=\"gpt-4o-mini\", n=3):\n",
    "    results = []\n",
    "\n",
    "    # Initialize the model and the chain\n",
    "    for i in tqdm(range(len(df))):\n",
    "        # Remove the document from the embedding\n",
    "        mask = df.index.isin([1])\n",
    "        contents = df[~mask].content\n",
    "        \n",
    "        chain = setup_langchain(contents, model)\n",
    "        \n",
    "        prompt = df.iloc[i].prompt\n",
    "        content = df['content'][i]\n",
    "        file_name = df['file_name'][i]\n",
    "\n",
    "        # Conduct n experiments\n",
    "        for experiment_num in tqdm(range(n), leave=False):\n",
    "            # Generate talking point\n",
    "            generated_talking_point = chain.invoke(prompt)\n",
    "\n",
    "            # Store result\n",
    "            # tppi_result = tppi.calculate_tppi(content, generated_talking_point)\n",
    "            \n",
    "            # Add metadata fields to each experiment result\n",
    "            result = {}\n",
    "            result['file_name'] = file_name\n",
    "            result['model'] = model\n",
    "            result['experiment_number'] = experiment_num + 1\n",
    "            result['content'] = content\n",
    "            result['prompt'] = prompt\n",
    "            result['generated_doc'] = generated_talking_point\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad41ac9-5c5f-4c37-a733-824de63b8eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "models = ['gpt-3.5-turbo', 'gpt-4o-mini', 'gpt-4o']\n",
    "all_results = []\n",
    "\n",
    "for model in models:\n",
    "    all_results.append(conduct_experiment(df, model, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2e00e2-7bc6-4a02-96f2-787017a2672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = [item for sublist in all_results for item in sublist]\n",
    "all_results = pd.DataFrame(flattened_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebf7333-64b2-42ad-ba02-5b175be07dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.to_csv('notebooks/RAG/model_exp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc43791-f6c1-4da4-b925-1c079237db3e",
   "metadata": {},
   "source": [
    "### 4.2 Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c650c8c-aefb-49cc-9a96-682683ce74c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_langchain(contents, llm_model=\"gpt-4o-mini\", temperature=0.5):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    texts = text_splitter.create_documents(contents)\n",
    "    splits = text_splitter.split_documents(texts)\n",
    "    \n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    vectorstore = FAISS.from_documents(texts, embedding=embeddings)\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    custom_prompt = PromptTemplate(\n",
    "        input_variables=[\"context\", \"question\"],\n",
    "        template=\"\"\"\n",
    "            You are an assistant specialized in creating Talking Points for United Nations. \n",
    "            Use the given context to create a the Talking Point.\n",
    "            The output should contain bullet points on each Talking Point.\n",
    "            If the context does not contain enough information, indicate that clearly.\n",
    "            \n",
    "            Question: {question}\n",
    "            Context: {context}\n",
    "            Answer:\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(model_name=llm_model, temperature=temperature)\n",
    "\n",
    "    chain = rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | custom_prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b381e62-0919-4f5f-8148-949df8dfe56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to conduct n experiments without averaging results\n",
    "def conduct_experiment(df, model=\"gpt-4o-mini\", temperature=0.5, n=3):\n",
    "    results = []\n",
    "\n",
    "    # Initialize the model and the chain\n",
    "    for i in tqdm(range(len(df))):\n",
    "        # Remove the document from the embedding\n",
    "        mask = df.index.isin([1])\n",
    "        contents = df[~mask].content\n",
    "        \n",
    "        chain = setup_langchain(contents, model, temperature)\n",
    "        \n",
    "        prompt = df.iloc[i].prompt\n",
    "        content = df['content'][i]\n",
    "        file_name = df['file_name'][i]\n",
    "\n",
    "        # Conduct n experiments\n",
    "        for experiment_num in tqdm(range(n), leave=False):\n",
    "            # Generate talking point\n",
    "            generated_talking_point = chain.invoke(prompt)\n",
    "\n",
    "            # Store result\n",
    "            # tppi_result = tppi.calculate_tppi(content, generated_talking_point)\n",
    "            \n",
    "            # Add metadata fields to each experiment result\n",
    "            result = {}\n",
    "            result['file_name'] = file_name\n",
    "            result['model'] = model\n",
    "            result['temperature'] = temperature\n",
    "            result['experiment_number'] = experiment_num + 1\n",
    "            result['content'] = content\n",
    "            result['prompt'] = prompt\n",
    "            result['generated_doc'] = generated_talking_point\n",
    "\n",
    "            results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebab06-3e15-4c6b-85a9-942451945e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run experiments\n",
    "temperatures = [0, 0.5, 1, 1.5, 2]\n",
    "all_results = []\n",
    "\n",
    "for temperature in temperatures:\n",
    "    all_results.append(conduct_experiment(df, model='gpt-4o', temperature=temperature, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ea5bca-b01b-4c32-9419-95eb19beef3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_data = [item for sublist in all_results for item in sublist]\n",
    "all_results = pd.DataFrame(flattened_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc6b41-af55-4654-96d9-12ef84030dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results.to_csv('notebooks/RAG/temperature_exp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33383ac-1c0a-4d84-b2a7-745c23551faa",
   "metadata": {},
   "source": [
    "## 5. Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9c2cf9-27f9-4fcc-aff5-8b3b0b49053e",
   "metadata": {},
   "source": [
    "### 5.1 Doc 17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a226d1c7-0256-4bdf-b1a6-b56eccf2e491",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df.drop(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c419b373-7583-4eaf-b39f-ce12dc4706b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents(df_aux['content'])\n",
    "splits = text_splitter.split_documents(texts)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b660378-e33c-4887-a707-19d98439a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(df.iloc[17].prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4434b4-7313-48c7-9148-79e7d760f270",
   "metadata": {},
   "source": [
    "### 5.2 Doc 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08628877-8690-4b3c-97d4-8737bb784a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df.drop(28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57b1fa1-078d-475e-aa8d-b030f739cf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents(df_aux['content'])\n",
    "splits = text_splitter.split_documents(texts)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c03579-9202-4f5e-a182-6f3f6657e4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(df.iloc[28].prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d310cff2-cb74-421d-91c6-b2a0e4691dd4",
   "metadata": {},
   "source": [
    "### 5.3 Doc 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf4e91-13ea-4870-9330-d32e20b6cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aux = df.drop(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81e0dd-0547-465e-8177-3c6baaa740ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents(df_aux['content'])\n",
    "splits = text_splitter.split_documents(texts)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77707c39-7f1e-4dad-b392-3498925612ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(df.iloc[25].prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cbc95-4653-4b9e-8cc1-4e7360eb1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[25].prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
